{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79456d6f-afb9-4b35-867c-295549c4e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv \n",
    "import cv2\n",
    "import gym\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import highway_env\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce41775f-8253-468b-851f-8653d19ee172",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Set parameter for training and file path\n",
    "    :param NO_ACTIONS_SIZE: Fast, slow, up, down, stable\n",
    "    :param DISCOUNT_FACTOR: The discount factor essentially determines how much the reinforcement learning agents cares about rewards\n",
    "    :param GENERATIONS: Iterations number\n",
    "    \"\"\"\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    GENERATIONS = 3000\n",
    "    NO_ACTIONS_SIZE = 5\n",
    "    IM_W, IM_H = 250, 160\n",
    "    DISCOUNT_FACTOR = 0.99\n",
    "    NODE_HISTORY_SIZE = 15000\n",
    "    SAVE_PATH = 'files/training/model_files/cp-{}.ckpt'\n",
    "    SAVE_DIR = os.path.dirname(SAVE_PATH)\n",
    "    LOG_PATH = os.path.join(\"files\", \"training\", \"my_logs\")\n",
    "    TF_WRITER = tf.summary.create_file_writer(\"files/training/my_logs/tf_board\")\n",
    "     \n",
    "    # Make directory to store log\n",
    "    def create_log_file():\n",
    "        if not os.path.exists(log_save_path):\n",
    "            os.makedirs(log_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f7e79d-a176-453e-b3b4-88c62c129035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network():\n",
    "    \"\"\"\n",
    "    I builded a network which have 9m parameter and use relu to activation function.\n",
    "    \"\"\"\n",
    "    inp = tf.keras.layers.Input((Config.IM_H, Config.IM_W, 4))\n",
    "    x = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(inp)\n",
    "    x = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv2D(64, (3,3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D((2,2))(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, (3,3), activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv2D(128, (3,3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D((2,2))(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(128, (3,3), activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv2D(256, (3,3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D((2,2))(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(512, (3,3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D((2,2))(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(Config.NO_ACTIONS_SIZE, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inp, outputs=x)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4959e9b-9510-4252-b9ce-5716b151f35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 160, 250, 4)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 158, 248, 32)      1184      \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 156, 246, 32)      9248      \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 154, 244, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 77, 122, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 77, 122, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 75, 120, 64)       36928     \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 73, 118, 128)      73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 36, 59, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 36, 59, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 34, 57, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 32, 55, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 16, 27, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 16, 27, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 14, 25, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 7, 12, 512)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 43008)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               22020608  \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 24,048,453\n",
      "Trainable params: 24,048,453\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 160, 250, 4)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 158, 248, 32)      1184      \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 156, 246, 32)      9248      \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 154, 244, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 77, 122, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 77, 122, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 75, 120, 64)       36928     \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 73, 118, 128)      73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 36, 59, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 36, 59, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 34, 57, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 32, 55, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 16, 27, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 16, 27, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 14, 25, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 7, 12, 512)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 43008)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               22020608  \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 24,048,453\n",
      "Trainable params: 24,048,453\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class DeepQnetwork:\n",
    "    \"\"\"\n",
    "    :param counter: We keep what step we are in \n",
    "    :param epsilon: Network hyper parameters\n",
    "    :param decay: Network hyper parameters\n",
    "    :param training/predict_network: \n",
    "    \"\"\"\n",
    "    counter=0\n",
    "    epsilon, min_epsilon = 1, 0.1\n",
    "    # Init predicting and training network\n",
    "    train_network = build_network()\n",
    "    predict_network = build_network()\n",
    "    decay = epsilon/((Config.GENERATIONS//2)-1)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7854d07-dd82-4529-a054-a86e0ba19533",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DeepQnetwork\n",
    "previous_memory = deque(maxlen=Config.NODE_HISTORY_SIZE) \n",
    "# Make environment\n",
    "env = gym.make('highway-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4497ceb3-4da5-4d92-b2e2-7170d16617c8",
   "metadata": {},
   "source": [
    "#### Set Enviroment Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cca3fe7-69a4-4c73-b4db-9ed6f8819b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The observations and actions of an environment are parametrized by a configuration, defined as a config dictionary.\n",
    "configr = {\n",
    "    \"offscreen_rendering\": True,\n",
    "    \"observation\": {\n",
    "        \"type\": \"GrayscaleObservation\",\n",
    "        \"weights\": [0.9, 0.1, 0.5],  # weights for RGB conversion\n",
    "        \"stack_size\": 4,\n",
    "        \"observation_shape\": (Config.IM_W, Config.IM_H)\n",
    "    },\n",
    "    \"screen_width\": Config.IM_W,\n",
    "    \"screen_height\": Config.IM_H,\n",
    "    \"scaling\": 5.75,\n",
    "    \"lanes_count\":4,\n",
    "}\n",
    "env.configure(configr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ae1f43-d5d3-404b-bc20-342583c517b1",
   "metadata": {},
   "source": [
    "#### Utils Function For Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6c476c8-ca46-4145-aecb-5ff48d0e23e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(states):\n",
    "    # Get prediction on from the predicted network actions using the states\n",
    "    states = np.reshape(states, newshape=(states.shape[0], Config.IM_H, Config.IM_W, 4))/255\n",
    "    prediction = dqn.predict_network(states)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def get_action(state):\n",
    "    # The function decides to randomly explore the action space or choose one from the predicted network actions.\n",
    "    # The epsilon value decides between exploration or exploitation mode of the agent and it is reduced over time.\n",
    "    if np.random.random() > dqn.epsilon:\n",
    "        _action = get_prediction(np.expand_dims(state, axis=0))\n",
    "        action = np.argmax(_action)\n",
    "    else:\n",
    "        action = np.random.randint(0, Config.NO_ACTIONS_SIZE)\n",
    "    return action\n",
    "\n",
    "def save_log(step, quantity, filename):\n",
    "    # The function save log to analyze result\n",
    "    with open(os.path.join(Config.LOG_PATH, filename), 'a+') as fi:\n",
    "        csv_w = csv.writer(fi, delimiter=',')\n",
    "        csv_w.writerow([step, quantity])\n",
    "\n",
    "\n",
    "def predict(states):\n",
    "    # Predict action using network from states, return 1x5 action probability list\n",
    "    # For example: return -> [0.85679,-0.00060,-0.00030,-0.00112,-0.00044]\n",
    "    states = np.reshape(states, newshape=(states.shape[0], Config.IM_H, Config.IM_W, 4))/255\n",
    "    prediction = dqn.train_network(states)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def update_q_value(rewards, current_q_list, next_q_list, actions, done):\n",
    "    # The function update q value to maximizing Qvalue for the policy\n",
    "    current_q_list = current_q_list.numpy()\n",
    "    next_max_qs = np.max(next_q_list, axis=1)\n",
    "    new_qs = rewards + (np.ones(done.shape)-done)*Config.DISCOUNT_FACTOR * next_max_qs\n",
    "    for i in range(len(current_q_list)):\n",
    "        current_q_list[i, actions[i]] = new_qs[i]\n",
    "    return current_q_list\n",
    "\n",
    "\n",
    "def loss_f(ground_truth, prediction):\n",
    "    # We use mean squared error to calculation loss between ground truth and prediction\n",
    "    loss = tf.keras.losses.mean_squared_error(ground_truth, prediction)\n",
    "    return loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(states, actions):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = dqn.train_network(states)\n",
    "        loss = loss_f(actions, predictions)\n",
    "    gradients = tape.gradient(loss, dqn.train_network.trainable_variables)\n",
    "    gradients = [tf.clip_by_norm(gradient, 10) for gradient in gradients]\n",
    "    dqn.optimizer.apply_gradients(zip(gradients, dqn.train_network.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def save_weigths(dqn):\n",
    "    # This function saves the model weights to the save_path\n",
    "    for train_grad, pred_grad in zip(dqn.train_network.trainable_variables, dqn.train_network.trainable_variables):\n",
    "        pred_grad.assign(train_grad)\n",
    "    dqn.train_network.save_weights(Config.SAVE_PATH.format(dqn.counter))\n",
    "    print(\"artis oldu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf854e-8b0b-4ea1-8188-e8391ce5cd46",
   "metadata": {},
   "source": [
    "#### Utils Function For Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69dc0965-a41c-4368-8a7c-da9e5268904a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_reward(observation, info):\n",
    "    # The reward value is returned according to the action.\n",
    "    if info['crashed']:\n",
    "        reward = -1\n",
    "    else:\n",
    "        if np.sum(observation[1:, 1]) > 0:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = 5\n",
    "    return reward\n",
    "\n",
    "\n",
    "def get_batch(sampling_size):\n",
    "    this_batch = random.sample(previous_memory, sampling_size)\n",
    "    current_nodes, actions, next_nodes, rewards, done = list(zip(*this_batch))\n",
    "    return [np.stack(current_nodes), np.array(actions), np.stack(next_nodes), np.array(rewards), np.array(done)]\n",
    "\n",
    "\n",
    "def train_network():\n",
    "    previous_memories = get_batch(Config.BATCH_SIZE)\n",
    "    dqn.counter += 1\n",
    "    current_nodes, actions, next_nodes, rewards, done = previous_memories\n",
    "    current_action_qs = predict(current_nodes)\n",
    "    next_action_qs = get_prediction(next_nodes)\n",
    "    current_action_qs = update_q_value(rewards, current_action_qs, next_action_qs, actions, done)\n",
    "    current_nodes = np.reshape(current_nodes, newshape=(Config.BATCH_SIZE, Config.IM_H, Config.IM_W, 4))/255\n",
    "\n",
    "    loss = train_step(current_nodes, current_action_qs)\n",
    "    \n",
    "    with Config.TF_WRITER.as_default():\n",
    "        tf.summary.scalar(\"loss\", data=np.mean(loss), step=dqn.counter)\n",
    "\n",
    "    save_log(dqn.counter, np.mean(loss), \"loss.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aed387-71db-4fbd-a29b-1781215d62b8",
   "metadata": {},
   "source": [
    "#### Run - Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57f4476-3ff9-477a-9921-3c66008517fc",
   "metadata": {},
   "outputs": [],
   "source": [
    " def run(episodes, train_frequency=2):\n",
    "    for episode in tqdm(range(episodes)):\n",
    "        \n",
    "        # Start enviroment Reader\n",
    "        observation = env.reset()\n",
    "        reward_history = []\n",
    "        step_counter = 0\n",
    "        \n",
    "        # Loop during duration time\n",
    "        while 1:\n",
    "            # our predicting network will predict actions for the given state \n",
    "            action = get_action(observation)\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "            # save the rewards that it got from taking the action\n",
    "            reward_history.append(reward)\n",
    "            # collect data to be saved in memory a\n",
    "            previous_memory.append([observation, action, next_observation, reward, 1 if done else 0])\n",
    "            observation = next_observation\n",
    "            \n",
    "            # Every tenth iteration, the training network will train on the data gathered in the memory. \n",
    "            # To avoid training too often.\n",
    "            if  step_counter%10 == 0:\n",
    "                if len(previous_memory) >= Config.BATCH_SIZE:\n",
    "                    train_network()\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # save result to analyze and show on tensorboard\n",
    "        with Config.TF_WRITER.as_default():\n",
    "            tf.summary.scalar(\"Episodic Average Rewards\", data=np.mean(reward_history), step=episode)\n",
    "            tf.summary.scalar(\"Epsilon\", data=dqn.epsilon, step=episode)\n",
    "        save_log(episode, np.mean(reward_history), \"episodic_reward.csv\")\n",
    "\n",
    "        if Config.GENERATIONS //2 >= episode >=1:\n",
    "            new_epsilon = dqn.epsilon-dqn.decay\n",
    "            dqn.epsilon = max(new_epsilon, dqn.min_epsilon)\n",
    "            \n",
    "        # For every 10th episode, we are updating our predicted network with our trained model from the locally saved directory.\n",
    "        if episode > 9 and episode % 10 == 0:\n",
    "            save_weigths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a9b35-8cf2-4ece-a1b6-3a6ea716d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "run(Config.GENERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d02503a-4df3-4c17-987b-41d7d16f8b8e",
   "metadata": {},
   "source": [
    "#### Run - Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24c891d3-0274-4b80-b0de-0091683d3316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create video capture to record game video\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(os.path.join(\"tmp\", 'output2.avi'),fourcc, 20.0, (Config.IM_W, Config.IM_H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5286ca4-f022-4b54-9ec4-580ca4ca382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(episodes, train_frequency=2):\n",
    "    \n",
    "    for episode in tqdm(range(episodes)):\n",
    "        # Start enviroment Reader and initlize reward_history/counter\n",
    "        observation = env.reset()\n",
    "        reward_history = []\n",
    "        step_counter = 0\n",
    "        \n",
    "        # Loop during duration time\n",
    "        while 1:\n",
    "            # Our predicting network predict actions for the given state \n",
    "            action = get_action(observation)\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "            reward_history.append(reward)\n",
    "            # Get frame to show and save video\n",
    "            frame = env.render(mode='rgb_array')\n",
    "            previous_memory.append([observation, action, next_observation, reward])\n",
    "            observation = next_observation\n",
    "            \n",
    "            # Frame is saved as video\n",
    "            out.write(frame)\n",
    "            \n",
    "            # Break loop if duration is completed\n",
    "            if done:\n",
    "                break\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9f6c10-1b06-4f7e-b28f-46708fc9539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test\n",
    "test(episodes=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
